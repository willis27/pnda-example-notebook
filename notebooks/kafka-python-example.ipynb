{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Kafka-Python with Jupyter Notebook#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka-Python Installation ##\n",
    "\n",
    "Log into Jupyter node (standard PNDA cluster) or edge node (pico PNDA cluster), and run the command below to install kafka-python:\n",
    "\n",
    "\n",
    " >`sudo su`\n",
    " \n",
    " >`export PATH=/opt/cloudera/parcels/Anaconda/bin:$PATH`\n",
    "\n",
    " >`conda install -c conda-forge kafka-python`\n",
    "\n",
    "\n",
    "Run the command below to verify the installation:\n",
    "\n",
    " > `conda list | grep 'kafka-python'`\n",
    " \n",
    " > `kafka-python              1.3.3                    py27_0    conda-forge`\n",
    " \n",
    "\n",
    "**<font color='red'> NOTE: you don't have to install kafka-python on each data nodes, as it will be visible by spark driver process only. **</font>\n",
    "\n",
    "In order to make python packages (e.g. kafka-python) available on data nodes or executor processes, you have two options: \n",
    "\n",
    "a) repeat the above steps on each data nodes (it is recommended only when you have multiple Spark apps share the same dependencies).\n",
    "\n",
    "b) ship python modules (.egg, .zip, .py) to each node in a spark session python package. Belows are example steps to build 'kafka-python' egg distribution. \n",
    "\n",
    "> `exit` # logout super user\n",
    "\n",
    "> `wget https://github.com/dpkp/kafka-python/archive/1.3.3.tar.gz`\n",
    "\n",
    "> `tar -zxvf 1.3.3.tar.gz`\n",
    "\n",
    "> `cd kafka-python-1.3.3`\n",
    "\n",
    "> `python setup.py bdist_egg`\n",
    "\n",
    "> `ls dist\n",
    "kafka_python-1.3.3-py2.7.egg`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run this cell only if you want to add python module to spark context and have run through steps of option b) \n",
    "sc.addPyFile(\"/home/ubuntu/kafka-python-1.3.3/dist/kafka_python-1.3.3-py2.7.egg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple producer test ##\n",
    "\n",
    "**Test Environment:**\n",
    "* PNDA release 3.4 (pico)\n",
    "* Kafka-Python: version 1.3.3\n",
    "* Kafka: version 0.10.0.1\n",
    "* Kafka Consumer: kafka console consumer\n",
    "* Kafka topic: 'test'\n",
    "\n",
    "Log on to kafka broker node and follow the steps below to create a 'test' topic if it does not exist:\n",
    "\n",
    "** [NOTE: you will need to find out zookeeper ip address and port (default port: 2181). You can get the information from Kafka manager portal link via PNDA portal.]**\n",
    "\n",
    " > `sudo /opt/pnda/kafka_2.11-0.10.0.1/bin/kafka-topics.sh --create --zookeeper <zookeeper_host_ip>:<zookeeper_service_port> --replication-factor 1 --partitions 1 --topic test`\n",
    "\n",
    " e.g.\n",
    " \n",
    " > `sudo /opt/pnda/kafka_2.11-0.10.0.1/bin/kafka-topics.sh --create --zookeeper 10.0.1.160:2181 --replication-factor 1 --partitions 1 --topic test`\n",
    "\n",
    " Check the 'test' topic is created successfully.\n",
    " \n",
    " >`kafka-topics.sh --list --zookeeper <zookeeper_host_ip>:<zookeeper_service_port>`\n",
    "\n",
    " e.g. \n",
    " \n",
    " >`sudo /opt/pnda/kafka_2.11-0.10.0.1/bin/kafka-topics.sh --list --zookeeper 10.0.1.160:2181\n",
    "__consumer_offsets\n",
    "avro.internal.testbot\n",
    "test`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define test kafka brokers and topics ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kafka_broker='10.0.1.160:9092' # replace argument with your kafka broker ip (if you have multiple brokers, pick one)\n",
    "topics = ('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple producer test ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from kafka import SimpleProducer\n",
    "from kafka import KafkaClient\n",
    "kafka=KafkaClient(kafka_broker) \n",
    "producer = SimpleProducer(kafka)\n",
    "producer.send_messages(topics, b'Hello From Kafka-Python Producer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple consumer test ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "consumer=KafkaConsumer(topics, bootstrap_servers=[kafka_broker], auto_offset_reset='earliest')\n",
    "for msg in consumer:\n",
    "    print(msg.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The End #"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark/Python2 (Anaconda)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
